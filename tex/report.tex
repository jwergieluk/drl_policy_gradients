\documentclass[a4paper,12pt]{amsart}
\usepackage[utf8]{inputenc}\usepackage[T1]{fontenc}

\title{Policy Gradient Algorithms}
\author{Julian Wergieluk}\address{}\email{julian.wergieluk@risklab.com}
\input{preamble}
\input{commands}

\usepackage[url=false,backend=biber]{biblatex}
\addbibresource{literature.bib}

\newcommand{\stateSpace}{\mathbb S}
\newcommand{\stateSpaceAlg}{\mathcal S}
\newcommand{\actionSpace}{\mathbb A}
\newcommand{\actionSpaceAlg}{\mathcal A}
\newcommand{\policy}{\pi}
\newcommand{\discountFactor}{\gamma}

\begin{document}

\maketitle

\begin{abstract}
This short note provides a concise description of the model architecture and
learning algorithms of the agent developed in this project. We also report learning
performance of the agent and provide a list of possible future model improvements.
\end{abstract}
\renewcommand*{\thefootnote}{}\footnote{\today{}}

\section{Description of the learning algorithm}

%Requirement: The report clearly describes the learning algorithm, along with
%the chosen hyperparameters. It also describes the model architectures for any
%neural networks.

\subsection{Summary of the notation}

We consider a Markov Decision Process with measurable state and action spaces
$(\stateSpace, \stateSpaceAlg)$ and $(\actionSpace, \actionSpaceAlg)$.
A policy $\policy$ is a Markov kernel from $(\stateSpace, \stateSpaceAlg)$ to
$(\actionSpace, \actionSpaceAlg)$, i.e.\ for each $s\in\stateSpace$, 
$\policy(. \vb s)$ is a probability distribution on $\actionSpaceAlg$.

The trajectory
\begin{align*}
    S_0, A_0, R_1, S_1, A_1, R_2, \ldots
\end{align*}
encodes the sequence of states, actions and resulting numerical rewards. The
rewards $R_i$ take values in $\R$. 
The return or discounted future reward $G_t$ at $t\in\N$ is 
\begin{align*}
    G_t &= \sum_{k=0}^{\infty} \discountFactor^{k} R_{t+k+1}.
\end{align*}

The dynamics of the environment is governed by the probability measure $P$
living in a probability space $(\Omega, \mathcal F)$. 
The central object of interest is the transition probability measure
$P(. \vb s, a)$ which is the distribution of $S_i$ under the conditions
$S_{i-1} = s$ and $A_{i-1} = a$ for fixed $s\in\stateSpace$ and $a\in\actionSpace$.

To simplify the setup, we assume that $R_i = \rho(S_i)$, i.e.\ the reward is a 
deterministic function of the state. Problems that do not fit into this 
setup can easily be transformed in ones that comply with it. 



\section{Training analysis}

\section{Ideas for future work}

%\nocite{meucci2009review, goodman2007}
\printbibliography

\end{document}


\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{{epsilon}.pdf}
    \caption{Decay of the $\varepsilon$ parameter as function the 
    training episode number.}
    \label{fig:epsilon}
\end{figure}

% vim: spelllang=en_us:spell:
